---
title: "p8105_hw5_kk3154"
author: "Kristen King"
date: "11/20/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  fig.width = 8, 
  fig.height = 6, 
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis", 
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1 - Homicide Data

The Washington Post collected homicide data from 50 cities in the US for an [article](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/) and spatial analysis on unsolved murders. They shared their data in a [github repository](https://github.com/washingtonpost/data-homicides).

Problem 1 works with the homicide data to estimate the unsolved homicide rate in each city. 

Downloading the homicide data, summarizing the raw data, creating a combined city_state variable, and summarizing the total number of homicides and unsolved homicides (disposition of "Closed without arrest" or "Open/No arrest") by cities:
```{r}
raw_hom_data = read_csv(file = "./Data/homicide_data.csv", show_col_types = FALSE)
skimr::skim(raw_hom_data)

homicide_df = read_csv(file = "./Data/homicide_data.csv", na = c("", "Unknown"), show_col_types = FALSE) %>% 
  mutate(
    city_state = str_c(city, state),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "TulsaAL")

homicide_df %>% 
  group_by(city_state) %>% 
  summarize(homicide_count = n(),
            unsolved_homicides = sum(resolution == "unsolved")) %>% 
  arrange(homicide_count) %>% 
  knitr::kable(
    col.names = c("City", "Count of  Homicides", "Count of Unsolved Homicides")
  )
```

For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "BaltimoreMD")
  
baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test=  
  prop.test(
    x = baltimore_summary %>% pull(unsolved),
    n = baltimore_summary %>% pull(n))

baltimore_test %>% 
  broom::tidy()
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of `purrr::map`, `purrr::map2`, list columns and `unnest` as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

Writing function and testing on a few cities:
```{r}
prop_test_function = function(city_df) {
  
  city_summary = 
    city_df %>% 
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n())
  
  city_test = 
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)

}

prop_test_function(baltimore_df)

homicide_df %>% 
  filter(city_state == "AlbuquerqueNM") %>% 
  prop_test_function()
```

Iterating across all cities:
```{r}
results_df = 
  homicide_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))

results_df
```

Alternate approach without creating a function, using map2:
```{r}
homicide_df %>% 
  group_by(city_state) %>% 
  summarize(homicide_count = n(),
            unsolved_homicides = sum(resolution == "unsolved")) %>% 
  mutate(
    test_results = map2(unsolved_homicides, homicide_count, prop.test),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
```

Create a plot that shows the estimates and CIs for each city – check out `geom_errorbar` for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2 - Longitudinal Study Data

Data from a longitudinal study that included a control arm and an experimental arm was downloaded and saved locally. Data for each participant is included in a separate file, and file names include the subject ID and arm.

Creating a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

```{r}
file_names = c(files = list.files("./Data/p2_zipdata/"))

csv_reading_func = function(df) {
  data = tibble(
    subject_id = substr(df, 1, 6), 
    study_arm = substr(df, 1, 3), 
    read_csv(str_c("./Data/p2_zipdata/", df), show_col_types = FALSE))
  return(data)
}

study_data = map_dfr(file_names, csv_reading_func) %>% 
  mutate(
    study_arm = case_when(
      study_arm == "con" ~ "control",
      study_arm == "exp" ~ "experimental"
    )
  ) %>% 
  pivot_longer(week_1:week_8,
    names_to = "study_week", 
    names_prefix = "week_"
  )

```

Making a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
study_data %>% 
  ggplot(aes(x = study_week, y = value)) +
  geom_line(aes(group = subject_id, color = study_arm)) + 
  labs(
    title = "Study Measurements Over Time", 
    y = "Study Value",
    x = "Study Week", 
    color = "Study Arm"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
```

There appears to be an increasing trend of study values over time among the subjects in the experimental arm of the study, but not in the control arm whose values remain about the same over the 8 week study period.

## Problem 3 - Writing a Function to Handle Missing Data

Loading iris data from tidyverse package and randomly introducing missing values:

```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Writing a function to replace these missing values with the mean continuous value or "virginica" species:

```{r}
fix_na = function(vector) {
  if (is.numeric(vector)) {
    vector = ifelse(is.na(vector), mean(vector, na.rm = TRUE), vector)
  }
  else if (is.character(vector)) {
    vector = ifelse(is.na(vector), "virginica", vector)
  }
  return(vector)
}

iris_missing_fixed = map_df(iris_with_missing, fix_na)

```

The number of missing entries in the `iris_with_missing` dataset is `r sum(is.na(iris_with_missing))`.

After applying my function to resolve missing data, the number of missing entries in the `iris_missing_fixed` dataset is `r sum(is.na(iris_missing_fixed))`.

```{r}
tibble(
  Dataset = c("iris_with_missing", "iris_missing_fixed"),
  n_missing = c(sum(is.na(iris_with_missing)), sum(is.na(iris_missing_fixed)))
) %>% 
knitr::kable()
```

